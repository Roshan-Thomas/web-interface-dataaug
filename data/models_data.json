{
    "common": {
        "bert-output": "The outputs which can be seen shows the highlighted words <span style='color:#7CFC00'>(in green)</span> that are changed by the model.",
        "aravec-output": "The outputs which can be seen shows the highlighted words <span style='color:#FFBF00'>(in yellow)</span> that are changed by the model."
    },
    "arabert": {
        "header": "### AraBERT Data Augmentation",
        "text": "AraBERT is an Arabic pre-trained language model based on Google's BERT architecture. BERT is a fully connected deep neural network trained to predict two main things: a masked word in a sentence and the probability that the two sentences flow with each other. We give BERT a sentence with a masked word and using the context of the sentence, BERT predicts the masked word.",
        "url": "aubmindlab/bert-large-arabertv2",
        "results": "Open to see AraBERT results"
    },
    "qarib-bert": {
        "header": "### QARiB Data Augmentation",
        "text": "QCRI Arabic and Dialectal BERT (QARiB) model, was trained on a collection of ~ 420 Million tweets and ~ 180 Million sentences of text. For the tweets, the data was collected using twitter API and using language filter. lang:ar. For the text data, it was a combination from Arabic GigaWord, Abulkhair Arabic Corpus and OPUS.",
        "url": "qarib/bert-base-qarib",
        "results": "Open to see QARiB results"
    },
    "xlm-roberta-bert": {
        "header": "### XLM-RoBERTa Data Augmentation",
        "text": "XLM-RoBERTa model pre-trained on 2.5TB of filtered CommonCrawl data containing 100 languages. It was introduced in the paper Unsupervised Cross-lingual Representation Learning at Scale by Conneau et al.",
        "url": "xlm-roberta-base",
        "results": "Open to see XLM-RoBERTa results"
    },
    "arabart": {
        "header": "### AraBART Data Augmentation",
        "text": "AraBART is the first Arabic model in which the encoder and the decoder are pretrained end-to-end, based on BART. AraBART follows the architecture of BART-Base which has 6 encoder and 6 decoder layers and 768 hidden dimensions. In total AraBART has 139M parameters.",
        "url": "moussaKam/AraBART",
        "results": "Open to see AraBART results"
    },
    "camelbert": {
        "header": "### CAMeLBERT-Mix Data Augmentation",
        "text": "CAMeLBERT-Mix NER Model is a Named Entity Recognition (NER) model that was built by fine-tuning the CAMeLBERT Mix model. For the fine-tuning, we used the ANERcorp dataset.",
        "url": "CAMeL-Lab/bert-base-arabic-camelbert-mix",
        "results": "Open to see CAMeLBERT-Mix results"
    },
    "bert-large-arabic": {
        "header": "### Arabic BERT (Large) Data Augmentation",
        "text": "Pretrained BERT Large language model for Arabic.",
        "url": "asafaya/bert-large-arabic",
        "results": "Open to see Arabic BERT (Large) results"
    },
    "ubc-arbert": {
        "header": "### ARBERT Data Augmentation",
        "text": "ARBERT is one of three models described in our ACl 2021 paper 'ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic'. ARBERT is a large-scale pre-trained masked language model focused on Modern Standard Arabic (MSA).",
        "url": "UBC-NLP/ARBERT",
        "results": "Open to see ARBERT results"
    },
    "ubc-marbertv2": {
        "header": "### MARBERTv2 Data Augmentation",
        "text": "",
        "url": "UBC-NLP/MARBERTv2",
        "results": "Open to see MARBERTv2 results"
    },
    "araelectra": {
        "header": "### AraELECTRA Data Augmentation",
        "text": "ELECTRA is a method for self-supervised language representation learning. It can be used to pre-train transformer networks using relatively little compute. ELECTRA models are trained to distinguish 'real' input tokens vs 'fake' input tokens generated by another neural network, similar to the discriminator of a GAN. AraELECTRA achieves state-of-the-art results on Arabic QA dataset.",
        "url": "aubmindlab/araelectra-base-generator",
        "results": "Open to see AraELECTRA results"
    },
    "aragpt2": {
        "header": "### AraGPT2 Data Augmentation",
        "text": "AraGPT2 is a pre-trained transformer for the Arabic Language Generation. The model successfully uses synthetic news generation and zero-shot question answering and has a 98% accuracy in detecting model-generated text. They are publicly available, and you can read the paper [here](https://arxiv.org/abs/2012.15520).",
        "url": "aubmindlab/aragpt2-medium",
        "results": "Open to see AraGPT2 results"
    },
    "aravec": {
        "header": "### AraVec (W2V) Data Augmentation",
        "text": "AraVec (W2V) is a pre-trained distributed word representation (word embedding) open-source project aiming to provide the Arabic NLP research community with free-to-use and powerful word embedding models. The recent version of AraVec provides 16-word embedding models built on top of two different Arabic content domains; Tweets and Wikipedia Arabic articles. This app uses the Twitter-trained model to augment the text. You can read more about it in this paper [here](https://www.researchgate.net/publication/319880027_AraVec_A_set_of_Arabic_Word_Embedding_Models_for_use_in_Arabic_NLP).",
        "results": "Open to see W2V results"
    },
    "double-back-translation": {
        "header": "### Double Back Translation Augmentation",
        "text": "The languages we are back translating from are: *English (en) [0,1], French (fr) [2,3], Turkish (tr) [4,5], Russian (ru) [6,7], Polish (pl) [8,9], Italian (it) [10,11], Spanish (es) [12,13], Greek (el) [14,15], German (de) [16,17], and Hebrew (he) [18,19]*.",
        "text-2": "For example, we are doing *AR > EN > AR_1 > EN > AR_2*. So AR_1 and AR_2 are the two augmented sentences giving us a much larger number of augmented sentences.",
        "results": "Open to see Back Translation results"
    },
    "m2m": {
        "header": "### Text-to-Text Augmentation",
        "text": "MBART is a sequence-to-sequence de-noising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. mBART is one of the first methods for pre-training a complete sequence-to-sequence model by de-noising full texts in multiple languages. At the same time, previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. You can read more about it from this paper [here](https://arxiv.org/abs/2001.08210).",
        "url": "facebook/mbart-large-50-many-to-many-mmt",
        "results": "Open to see Text-to-Text Results"
    }
}